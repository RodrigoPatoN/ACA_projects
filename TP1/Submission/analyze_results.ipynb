{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dnn = pd.read_json('results_dnn.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dnn = []\n",
    "\n",
    "for i, row in data_dnn.iterrows():\n",
    "\n",
    "    accuracy_fold_train = []\n",
    "    accuracy_fold_test = []\n",
    "\n",
    "    recall_fold_train = []\n",
    "    recall_fold_test = []\n",
    "\n",
    "    precision_fold_train = []\n",
    "    precision_fold_test = []\n",
    "\n",
    "    for iteration in row[\"results\"]:\n",
    "        \n",
    "        loss = iteration[\"loss_values\"]\n",
    "        conf_mat_train = np.array(iteration[\"confusion_matrix_train\"])\n",
    "        conf_mat_test = np.array(iteration[\"confusion_matrix_val\"])\n",
    "        \n",
    "        # Compute accuracy (7 classes)\n",
    "        total_train = conf_mat_train.sum()\n",
    "        total_test = conf_mat_test.sum()\n",
    "\n",
    "        accuracy_train = conf_mat_train.diagonal().sum() / total_train if total_train > 0 else 0\n",
    "        accuracy_test = conf_mat_test.diagonal().sum() / total_test if total_test > 0 else 0\n",
    "\n",
    "        # Compute recall and precision (per class)\n",
    "        recall_train = np.array([\n",
    "            conf_mat_train[i, i] / conf_mat_train[i, :].sum() if conf_mat_train[i, :].sum() > 0 else 0\n",
    "            for i in range(7)\n",
    "        ])\n",
    "        recall_test = np.array([\n",
    "            conf_mat_test[i, i] / conf_mat_test[i, :].sum() if conf_mat_test[i, :].sum() > 0 else 0\n",
    "            for i in range(7)\n",
    "        ])\n",
    "\n",
    "        precision_train = np.array([\n",
    "            conf_mat_train[i, i] / conf_mat_train[:, i].sum() if conf_mat_train[:, i].sum() > 0 else 0\n",
    "            for i in range(7)\n",
    "        ])\n",
    "        precision_test = np.array([\n",
    "            conf_mat_test[i, i] / conf_mat_test[:, i].sum() if conf_mat_test[:, i].sum() > 0 else 0\n",
    "            for i in range(7)\n",
    "        ])\n",
    "\n",
    "        accuracy_fold_train.append(accuracy_train)\n",
    "        accuracy_fold_test.append(accuracy_test)\n",
    "\n",
    "        recall_fold_train.append(recall_train)\n",
    "        recall_fold_test.append(recall_test)\n",
    "\n",
    "        precision_fold_train.append(precision_train)\n",
    "        precision_fold_test.append(precision_test)\n",
    "\n",
    "    # Compute mean across folds\n",
    "    accuracy_train = np.mean(accuracy_fold_train)\n",
    "    accuracy_test = np.mean(accuracy_fold_test)\n",
    "\n",
    "    recall_train = np.mean(recall_fold_train, axis=0)\n",
    "    recall_test = np.mean(recall_fold_test, axis=0)\n",
    "\n",
    "    precision_train = np.mean(precision_fold_train, axis=0)\n",
    "    precision_test = np.mean(precision_fold_test, axis=0)\n",
    "\n",
    "    average_recall_train = np.mean(recall_train)\n",
    "    average_recall_test = np.mean(recall_test)\n",
    "\n",
    "    average_precision_train = np.mean(precision_train)\n",
    "    average_precision_test = np.mean(precision_test)\n",
    "\n",
    "    f1_train_average = 2 * (average_precision_train * average_recall_train) / (average_precision_train + average_recall_train)\n",
    "    f1_test_average = 2 * (average_precision_test * average_recall_test) / (average_precision_test + average_recall_test) \n",
    "\n",
    "    row = row.drop(\"results\")\n",
    "    row_dict = row.to_dict()\n",
    "\n",
    "    metrics_dnn.append({\n",
    "        **row_dict,\n",
    "        \"iteration\": i,\n",
    "        \"accuracy_train\": accuracy_train,\n",
    "        \"accuracy_test\": accuracy_test,\n",
    "        \"recall_train\": recall_train.tolist(),  # Convert to list for DataFrame compatibility\n",
    "        \"recall_test\": recall_test.tolist(),\n",
    "        \"averaged_recall_train\": average_recall_train,\n",
    "        \"averaged_recall_test\": average_recall_test,\n",
    "        \"precision_train\": precision_train.tolist(),\n",
    "        \"precision_test\": precision_test.tolist(),\n",
    "        \"averaged_precision_train\": average_precision_train,\n",
    "        \"averaged_precision_test\": average_precision_test,\n",
    "        \"f1_train_average\": f1_train_average,\n",
    "        \"f1_test_average\": f1_test_average\n",
    "    })\n",
    "\n",
    "metrics_dnn = pd.DataFrame(metrics_dnn)\n",
    "#metrics_dnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(data : pd.DataFrame):\n",
    "\n",
    "    metrics_cnn = []\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "\n",
    "        accuracy_fold_train = []\n",
    "        accuracy_fold_test = []\n",
    "\n",
    "        recall_fold_train = []\n",
    "        recall_fold_test = []\n",
    "\n",
    "        precision_fold_train = []\n",
    "        precision_fold_test = []\n",
    "\n",
    "        confusion_matrix_train = []\n",
    "        confusion_matrix_test = []\n",
    "\n",
    "        all_losses = []\n",
    "        all_losses_val = []\n",
    "\n",
    "        for iteration in row[\"results\"]:\n",
    "            \n",
    "            loss = iteration[\"loss_values\"]\n",
    "            try:\n",
    "                val_loss = iteration[\"val_loss\"]\n",
    "            except:\n",
    "                val_loss = iteration[\"loss_values\"]\n",
    "\n",
    "            all_losses.append(loss)\n",
    "            all_losses_val.append(val_loss)\n",
    "\n",
    "            conf_mat_train = np.array(iteration[\"confusion_matrix_train\"])\n",
    "            conf_mat_test = np.array(iteration[\"confusion_matrix_val\"])\n",
    "            \n",
    "            # Compute accuracy (7 classes)\n",
    "            total_train = conf_mat_train.sum()\n",
    "            total_test = conf_mat_test.sum()\n",
    "\n",
    "            accuracy_train = conf_mat_train.diagonal().sum() / total_train if total_train > 0 else 0\n",
    "            accuracy_test = conf_mat_test.diagonal().sum() / total_test if total_test > 0 else 0\n",
    "\n",
    "            # Compute recall and precision (per class)\n",
    "            recall_train = np.array([\n",
    "                conf_mat_train[i, i] / conf_mat_train[i, :].sum() if conf_mat_train[i, :].sum() > 0 else 0\n",
    "                for i in range(7)\n",
    "            ])\n",
    "            recall_test = np.array([\n",
    "                conf_mat_test[i, i] / conf_mat_test[i, :].sum() if conf_mat_test[i, :].sum() > 0 else 0\n",
    "                for i in range(7)\n",
    "            ])\n",
    "\n",
    "            precision_train = np.array([\n",
    "                conf_mat_train[i, i] / conf_mat_train[:, i].sum() if conf_mat_train[:, i].sum() > 0 else 0\n",
    "                for i in range(7)\n",
    "            ])\n",
    "            precision_test = np.array([\n",
    "                conf_mat_test[i, i] / conf_mat_test[:, i].sum() if conf_mat_test[:, i].sum() > 0 else 0\n",
    "                for i in range(7)\n",
    "            ])\n",
    "\n",
    "            try:\n",
    "                confusion_matrix_train += conf_mat_train\n",
    "                confusion_matrix_test += conf_mat_test\n",
    "            except:\n",
    "                confusion_matrix_train = conf_mat_train\n",
    "                confusion_matrix_test = conf_mat_test\n",
    "\n",
    "            accuracy_fold_train.append(accuracy_train)\n",
    "            accuracy_fold_test.append(accuracy_test)\n",
    "\n",
    "            recall_fold_train.append(recall_train)\n",
    "            recall_fold_test.append(recall_test)\n",
    "\n",
    "            precision_fold_train.append(precision_train)\n",
    "            precision_fold_test.append(precision_test)\n",
    "\n",
    "        # Compute mean across folds\n",
    "        accuracy_train = np.mean(accuracy_fold_train)\n",
    "        accuracy_test = np.mean(accuracy_fold_test)\n",
    "\n",
    "        recall_train = np.mean(recall_fold_train, axis=0)\n",
    "        recall_test = np.mean(recall_fold_test, axis=0)\n",
    "\n",
    "        precision_train = np.mean(precision_fold_train, axis=0)\n",
    "        precision_test = np.mean(precision_fold_test, axis=0)\n",
    "\n",
    "        average_recall_train = np.mean(recall_train)\n",
    "        average_recall_test = np.mean(recall_test)\n",
    "\n",
    "        average_precision_train = np.mean(precision_train)\n",
    "        average_precision_test = np.mean(precision_test)\n",
    "\n",
    "        f1_train_average = 2 * (average_precision_train * average_recall_train) / (average_precision_train + average_recall_train)\n",
    "        f1_test_average = 2 * (average_precision_test * average_recall_test) / (average_precision_test + average_recall_test) \n",
    "\n",
    "        row = row.drop(\"results\")\n",
    "        row_dict = row.to_dict()\n",
    "\n",
    "        metrics_cnn.append({\n",
    "            **row_dict,\n",
    "            \"iteration\": i,\n",
    "            \"accuracy_train\": accuracy_train,\n",
    "            \"accuracy_test\": accuracy_test,\n",
    "            \"recall_train\": recall_train.tolist(),  # Convert to list for DataFrame compatibility\n",
    "            \"recall_test\": recall_test.tolist(),\n",
    "            \"averaged_recall_train\": average_recall_train,\n",
    "            \"averaged_recall_test\": average_recall_test,\n",
    "            \"precision_train\": precision_train.tolist(),\n",
    "            \"precision_test\": precision_test.tolist(),\n",
    "            \"averaged_precision_train\": average_precision_train,\n",
    "            \"averaged_precision_test\": average_precision_test,\n",
    "            \"f1_train_average\": f1_train_average,\n",
    "            \"f1_test_average\": f1_test_average,\n",
    "            \"confusion_matrix_train\": confusion_matrix_train,\n",
    "            \"confusion_matrix_test\": confusion_matrix_test,\n",
    "            \"loss\": all_losses,\n",
    "            \"val_loss\": all_losses_val\n",
    "        })\n",
    "\n",
    "    metrics_cnn = pd.DataFrame(metrics_cnn)\n",
    "    return metrics_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df):\n",
    "    confusion_matrix_train = np.array(df[\"confusion_matrix_train\"].tolist())\n",
    "    confusion_matrix_test = np.array(df[\"confusion_matrix_test\"].tolist())\n",
    "\n",
    "    # Convert values to strings for text display\n",
    "    labels_train = [[str(val) for val in row] for row in confusion_matrix_train]\n",
    "    labels_test = [[str(val) for val in row] for row in confusion_matrix_test]\n",
    "\n",
    "    classes = [\"Class {}\".format(i) for i in range(7)]\n",
    "\n",
    "    # Create subplot layout: 1 row, 2 columns\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Train\", \"Validation\")\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=confusion_matrix_train,\n",
    "            x=classes,\n",
    "            y=classes,\n",
    "            colorscale='Viridis',\n",
    "            text=labels_train,\n",
    "            texttemplate=\"%{text}\",\n",
    "            showscale=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=confusion_matrix_test,\n",
    "            x=classes,\n",
    "            y=classes,\n",
    "            colorscale='Viridis',\n",
    "            text=labels_test,\n",
    "            texttemplate=\"%{text}\",\n",
    "            showscale=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1200,\n",
    "        font=dict(size=16),\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Predicted Class\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"True Class\", row=1, col=1)\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Predicted Class\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"True Class\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn = pd.read_json('results_cnn_1.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_cnn = get_metrics(data_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 50 best performances in terms of test accuracy\n",
    "best_cnn = metrics_cnn.sort_values(by=\"accuracy_test\", ascending=False).head(50)\n",
    "data_cnn_50_best = data_cnn.loc[best_cnn.index]\n",
    "data_cnn_50_best.to_csv(\"data_cnn_50_best.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of the different Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dnn[\"model\"] = \"DNN\"\n",
    "metrics_cnn[\"model\"] = \"CNN\"\n",
    "\n",
    "metrics_all = pd.concat([metrics_cnn, metrics_dnn], ignore_index=True)\n",
    "metrics_all[\"learning_rate\"] = metrics_all[\"learning_rate\"].astype(str)\n",
    "metrics_all[\"batch_size\"] = metrics_all[\"batch_size\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = ['activation_function', 'n_layers', 'batch_size', \n",
    "                'learning_rate', 'loss_function', 'optimizer',\n",
    "                'pooling', 'n_conv_layers']\n",
    "\n",
    "metrics = ['accuracy_train', 'accuracy_test']\n",
    "\n",
    "metrics_to_str = {\n",
    "    'accuracy_train': 'Accuracy (Train)',\n",
    "    'accuracy_test': 'Accuracy (Validation)'\n",
    "}\n",
    "\n",
    "# Grid layout settings\n",
    "params_per_row = 2  # Two parameters per row\n",
    "n_rows = -(-len(hyperparameters) // params_per_row)  # Ceiling division for rows\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=n_rows, cols=params_per_row * 2,  # Each parameter has 2 charts (train & test)\n",
    "    subplot_titles=[f\"{param} - {metrics_to_str[metric]}\" for param in hyperparameters for metric in metrics],\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.05, vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Loop through each hyperparameter\n",
    "for i, param in enumerate(hyperparameters):\n",
    "    row = i // params_per_row + 1  # Compute row index\n",
    "\n",
    "    df_grouped = metrics_all.groupby([param, \"model\"])[metrics].mean().reset_index()\n",
    "\n",
    "    # Add plots for accuracy_train and accuracy_test\n",
    "    for j, metric in enumerate(metrics):\n",
    "\n",
    "        col = i % params_per_row * 2 + j + 1  # Compute column index\n",
    "\n",
    "        for model, color in zip([\"DNN\", \"CNN\"], [\"#636EFA\", \"#EF553B\"]):\n",
    "\n",
    "            df_subset = df_grouped[df_grouped[\"model\"] == model]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=df_subset[param], \n",
    "                    y=df_subset[metric], \n",
    "                    name=f\"{model}\",\n",
    "                    marker_color=color,\n",
    "                    legendgroup=model,\n",
    "                    showlegend=(i == 0 and j == 0)  # Show legend only once\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Impact of Hyperparameters on Accuracy (CNN vs. DNN)\",\n",
    "    height=300 * n_rows, width=1400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the configuration with the best test accuracy\n",
    "\n",
    "best_cnn = metrics_cnn.sort_values(by=\"accuracy_test\", ascending=False).head(1)\n",
    "best_dnn = metrics_dnn.sort_values(by=\"accuracy_test\", ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_params = best_cnn.drop(columns=[\"iteration\", 'accuracy_train', 'accuracy_test', 'recall_train',\n",
    "       'recall_test', 'averaged_recall_train', 'averaged_recall_test',\n",
    "       'precision_train', 'precision_test', 'averaged_precision_train',\n",
    "       'averaged_precision_test', 'f1_train_average', 'f1_test_average',\n",
    "       \"model\"])\n",
    "\n",
    "print(best_cnn.columns)\n",
    "best_cnn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot with train and test accuracy\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[\"CNN\", \"DNN\"],\n",
    "    y=[best_cnn[\"accuracy_train\"].values[0], best_dnn[\"accuracy_train\"].values[0]],\n",
    "    name='Train',\n",
    "    marker_color='#636EFA'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[\"CNN\", \"DNN\"],\n",
    "    y=[best_cnn[\"accuracy_test\"].values[0], best_dnn[\"accuracy_test\"].values[0]],\n",
    "    name='Validation',\n",
    "    marker_color='#EF553B'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Train and Test Accuracy for the Best CNN and DNN Models',\n",
    "    barmode='group',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Accuracy',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot with train and test accuracy\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[\"CNN\", \"DNN\"],\n",
    "    y=[best_cnn[\"f1_train_average\"].values[0], best_dnn[\"f1_train_average\"].values[0]],\n",
    "    name='Train',\n",
    "    marker_color='#636EFA'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[\"CNN\", \"DNN\"],\n",
    "    y=[best_cnn[\"f1_test_average\"].values[0], best_dnn[\"f1_test_average\"].values[0]],\n",
    "    name='Validation',\n",
    "    marker_color='#EF553B'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Train and Test Accuracy for the Best CNN and DNN Models',\n",
    "    barmode='group',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Accuracy',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_cnn.loc[metrics_cnn[\"accuracy_test\"].idxmax()]\n",
    "\n",
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXING ATTEMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn_early_stopping = pd.read_json('results_cnn_early.json').T\n",
    "#data_cnn_early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_early = get_metrics(data_cnn_early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_train = metrics_early[\"f1_train_average\"].max()\n",
    "max_f1_test = metrics_early[\"f1_test_average\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_aug_cnn = pd.read_json('../results_original/results_cnn.json').T\n",
    "\n",
    "#data_no_aug_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "results_all = []\n",
    "j = 0\n",
    "\n",
    "for i, row in data_no_aug_cnn.iterrows():\n",
    "\n",
    "    conf_mat_train = np.array(row[\"confusion_matrix_train\"])\n",
    "    conf_mat_test = np.array(row[\"confusion_matrix_val\"])\n",
    "\n",
    "    total_train = conf_mat_train.sum()\n",
    "    total_test = conf_mat_test.sum()\n",
    "\n",
    "    accuracy_train = conf_mat_train.diagonal().sum() / total_train if total_train > 0 else 0\n",
    "    accuracy_test = conf_mat_test.diagonal().sum() / total_test if total_test > 0 else 0\n",
    "\n",
    "    # Compute recall and precision (per class)\n",
    "    recall_train = np.array([\n",
    "        conf_mat_train[i, i] / conf_mat_train[i, :].sum() if conf_mat_train[i, :].sum() > 0 else 0\n",
    "        for i in range(7)\n",
    "    ])\n",
    "    recall_test = np.array([\n",
    "        conf_mat_test[i, i] / conf_mat_test[i, :].sum() if conf_mat_test[i, :].sum() > 0 else 0\n",
    "        for i in range(7)\n",
    "    ])\n",
    "\n",
    "    precision_train = np.array([\n",
    "        conf_mat_train[i, i] / conf_mat_train[:, i].sum() if conf_mat_train[:, i].sum() > 0 else 0\n",
    "        for i in range(7)\n",
    "    ])\n",
    "    precision_test = np.array([\n",
    "        conf_mat_test[i, i] / conf_mat_test[:, i].sum() if conf_mat_test[:, i].sum() > 0 else 0\n",
    "        for i in range(7)\n",
    "    ])\n",
    "\n",
    "    average_recall_train = np.mean(recall_train)\n",
    "    average_precision_train = np.mean(precision_train)\n",
    "\n",
    "    average_recall_test = np.mean(recall_test)\n",
    "    average_precision_test = np.mean(precision_test)\n",
    "\n",
    "    f1_score_train = 2 * (average_precision_train * average_recall_train) / (average_precision_train + average_recall_train)\n",
    "    f1_score_test  = 2 * (average_precision_test  * average_recall_test)  / (average_precision_test  + average_recall_test)\n",
    "\n",
    "    results_all.append({\n",
    "        \"iteration\": j,\n",
    "        \"accuracy_train\": accuracy_train,\n",
    "        \"accuracy_test\": accuracy_test,\n",
    "        \"recall_train\": recall_train.tolist(),\n",
    "        \"recall_test\": recall_test.tolist(),\n",
    "        \"averaged_recall_train\": average_precision_train,\n",
    "        \"averaged_recall_test\": average_recall_test,\n",
    "        \"precision_train\": precision_train,\n",
    "        \"precision_test\": precision_test,\n",
    "        \"averaged_precision_train\": average_precision_train,\n",
    "        \"averaged_precision_test\": average_precision_test,\n",
    "        \"f1_score_train\": f1_score_train,\n",
    "        \"f1_score_test\": f1_score_test,\n",
    "        \"confusion_matrix_train\": conf_mat_train,\n",
    "        \"confusion_matrix_test\": conf_mat_test,\n",
    "    })\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "metrics_cnn_no_aug = pd.DataFrame(results_all)\n",
    "#metrics_cnn_no_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_train = metrics_cnn_no_aug[\"f1_score_train\"].max()\n",
    "max_f1_test = metrics_cnn_no_aug[\"f1_score_test\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_cnn_no_aug.loc[metrics_cnn_no_aug[\"f1_score_test\"].idxmax()]\n",
    "\n",
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_under_cnn = pd.read_json('./results_cnn_under.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_under_cnn = get_metrics(data_under_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_train = metrics_under_cnn[\"f1_train_average\"].max()\n",
    "max_f1_test = metrics_under_cnn[\"f1_test_average\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_under_cnn.loc[metrics_under_cnn[\"f1_test_average\"].idxmax()]\n",
    "\n",
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_early_cnn = pd.read_json('./results_cnn_early.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_early_cnn = get_metrics(data_early_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_train = metrics_early_cnn[\"f1_train_average\"].max()\n",
    "max_f1_test = metrics_early_cnn[\"f1_test_average\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_early_cnn.loc[metrics_early_cnn[\"f1_test_average\"].idxmax()]\n",
    "\n",
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_decay_cnn = pd.read_json('./results_cnn_decay.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_decay_cnn = get_metrics(data_decay_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_test = metrics_decay_cnn[\"f1_test_average\"].max()\n",
    "max_f1_train = metrics_decay_cnn[\"f1_train_average\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_decay_cnn.loc[metrics_decay_cnn[\"f1_test_average\"].idxmax()]\n",
    "\n",
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tests\n",
    "\n",
    "None of the previous solutions worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_add_cnn = pd.read_json('./results_cnn_additional.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_add_cnn = get_metrics(data_add_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_test = metrics_add_cnn[\"f1_test_average\"].max()\n",
    "max_f1_train = metrics_add_cnn[\"f1_train_average\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_add_cnn.loc[metrics_add_cnn[\"f1_test_average\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for best f1 score\n",
    "best_f1_test = metrics_add_cnn[\"f1_test_average\"].max()\n",
    "best_f1_test_row = metrics_add_cnn.loc[metrics_add_cnn[\"f1_test_average\"].idxmax()]\n",
    "best_f1_test_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a line plot with all the losses - different color for each fold\n",
    "\n",
    "all_losses = best_f1_test_row[\"loss\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, loss in enumerate(all_losses):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(loss))),\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name=f'Fold {i+1}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Losses for Each Fold',\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Folds'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extensive_cnn = pd.read_json('./results_cnn_extensive.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_extensive_cnn = get_metrics(data_extensive_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_test = metrics_extensive_cnn[\"f1_test_average\"].max()\n",
    "max_f1_train = metrics_extensive_cnn[\"f1_train_average\"].max()\n",
    "\n",
    "max_f1_test, max_f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for best f1 score\n",
    "best_f1_test = metrics_extensive_cnn[\"f1_test_average\"].max()\n",
    "best_f1_test_row = metrics_extensive_cnn.loc[metrics_extensive_cnn[\"f1_test_average\"].idxmax()]\n",
    "best_f1_test_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a line plot with all the losses - different color for each fold\n",
    "\n",
    "all_losses = best_f1_test_row[\"loss\"]\n",
    "all_losses_val = best_f1_test_row[\"val_loss\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, loss in enumerate(all_losses):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(loss))),\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name=f'Fold {i+1}'\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Folds',\n",
    "    font=dict(size=16),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses_val = best_f1_test_row[\"val_loss\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, loss in enumerate(all_losses_val):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(loss))),\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name=f'Fold {i+1}'\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Folds',\n",
    "    font=dict(size=16),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_f1 = metrics_extensive_cnn.loc[metrics_extensive_cnn[\"f1_test_average\"].idxmax()]\n",
    "\n",
    "plot_confusion_matrix(params_max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn = pd.read_json('./results_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = []\n",
    "\n",
    "for i, row in test_cnn.iterrows():\n",
    "\n",
    "    conf_mat_row = row[\"confusion_matrix_test\"]\n",
    "    conf_mat.append(conf_mat_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert values to strings for text display\n",
    "labels_train = [[str(val) for val in row] for row in conf_mat]\n",
    "\n",
    "classes = [\"Class {}\".format(i) for i in range(7)]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=conf_mat,\n",
    "        x=classes,\n",
    "        y=classes,\n",
    "        colorscale='Viridis',\n",
    "        text=labels_train,\n",
    "        texttemplate=\"%{text}\",\n",
    "        showscale=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    font=dict(size=16),\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Predicted Class\")\n",
    "fig.update_yaxes(title_text=\"True Class\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = np.array(conf_mat)\n",
    "\n",
    "recall_test = np.array([\n",
    "        conf_mat[i, i] / conf_mat[i, :].sum() if conf_mat[i, :].sum() > 0 else 0 for i in range(7)\n",
    "    ])\n",
    "\n",
    "precision_test = np.array([\n",
    "    conf_mat[i, i] / conf_mat[:, i].sum() if conf_mat[:, i].sum() > 0 else 0\n",
    "    for i in range(7)\n",
    "])\n",
    "\n",
    "average_recall_test = np.mean(recall_test)\n",
    "average_precision_test = np.mean(precision_test)\n",
    "\n",
    "f1_score_test = 2 * (average_precision_test * average_recall_test) / (average_precision_test + average_recall_test)\n",
    "\n",
    "total_test = conf_mat.sum()\n",
    "accuracy_test = conf_mat.diagonal().sum() / total_test if total_test > 0 else 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
